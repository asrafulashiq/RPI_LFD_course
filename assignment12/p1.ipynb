{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from copy import deepcopy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "theta = np.tanh  # activation function\n",
    "theta_prime = lambda x: 1 - np.tanh(x)**2  # derivative of the activation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# forward pass\n",
    "x = np.array([1,2])\n",
    "y = 1\n",
    "\n",
    "class NNLayer:\n",
    "    def __init__(self,\n",
    "                 dims=[2, 2, 1],\n",
    "                 out_theta=np.tanh,\n",
    "                 out_theta_prime=lambda x: 1 - np.tanh(x)**2,\n",
    "                 theta=np.tanh,\n",
    "                 theta_prime=lambda x: 1 - np.tanh(x)**2) -> None:\n",
    "        self.W = []\n",
    "        for i in range(len(dims)-1):\n",
    "            self.W.append(0.25 * np.ones((dims[i]+1, dims[i+1])))\n",
    "\n",
    "        self.out_theta = out_theta\n",
    "        self.out_theta_prime = out_theta_prime\n",
    "        self.theta = theta\n",
    "        self.theta_prime = theta_prime\n",
    "\n",
    "    def forward(self, x, W=None):\n",
    "        # first forward pass\n",
    "        if W is None:\n",
    "            W = self.W\n",
    "        num_layer = len(W)\n",
    "        a = x\n",
    "        self.input = x\n",
    "        s_list = []\n",
    "        a_list = []\n",
    "        for i in range(num_layer):\n",
    "            s = np.dot(np.append(a, 1), W[i])\n",
    "            if i == len(W) - 1:\n",
    "                a = self.out_theta(s)\n",
    "            else:\n",
    "                a = self.theta(s)\n",
    "            s_list.append(s)\n",
    "            a_list.append(a)\n",
    "\n",
    "        self.a_list = a_list\n",
    "        self.s_list = s_list\n",
    "        return a\n",
    "\n",
    "    def calc_loss(self, pred, y):\n",
    "        # calculate loss\n",
    "        e_in = np.mean(1/4. * (pred - y)**2)\n",
    "        return e_in\n",
    "\n",
    "    def calc_grad(self, x, y):\n",
    "        pred = self.forward(x)\n",
    "\n",
    "        self.d_list = []\n",
    "\n",
    "        # compute sensitivity\n",
    "        d = 1. / 4 * 2 * (pred - y) * self.out_theta_prime(self.s_list[-1])\n",
    "        self.d_list.append(d)\n",
    "        for l in range(len(self.W) - 2, -1, -1):\n",
    "            # print(l)\n",
    "            d = self.W[l+1][:-1].dot(d) * self.theta_prime(self.s_list[l])\n",
    "            self.d_list.append(d)\n",
    "        self.d_list = list(reversed(self.d_list))\n",
    "\n",
    "        # gradient descent: calculate gradient G1, G2\n",
    "        self.G_list = []\n",
    "        for i in range(len(self.W)):\n",
    "            if i == 0:\n",
    "                a = self.input\n",
    "            else:\n",
    "                a = self.a_list[i-1]\n",
    "            a = np.append(a, 1)\n",
    "            G = a[:, None] @ self.d_list[i][:, None].T\n",
    "            self.G_list.append(G)\n",
    "            # G2 = a1[:, None] @ d2[:, None].T\n",
    "        return self.G_list\n",
    "\n",
    "    def calc_grad_numerical(self, x, y, eps=1e-4):\n",
    "        G_list = []\n",
    "\n",
    "        for l in range(len(self.W)):\n",
    "            r, c = self.W[l].shape\n",
    "            G = np.zeros((r, c))\n",
    "            for i in range(r):\n",
    "                for j in range(c):\n",
    "                    W_copy = deepcopy(self.W)\n",
    "                    W_copy[l][i, j] += eps\n",
    "                    pred_plus = self.forward(x, W_copy)\n",
    "                    loss_plus = self.calc_loss(pred_plus, y)\n",
    "                    W_copy[l][i, j] -= 2 * eps\n",
    "                    pred_minus = self.forward(x, W_copy)\n",
    "                    loss_minus = self.calc_loss(pred_minus, y)\n",
    "                    G[i, j] = (loss_plus - loss_minus) / (2 * eps)\n",
    "            G_list.append(G)\n",
    "        return G_list\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.01594156 -0.01594156]\n",
      " [-0.03188311 -0.03188311]\n",
      " [-0.01594156 -0.01594156]]\n",
      "[[-0.1156356 ]\n",
      " [-0.1156356 ]\n",
      " [-0.15183362]]\n",
      "[[-0.01594156 -0.01594156]\n",
      " [-0.03188311 -0.03188311]\n",
      " [-0.01594156 -0.01594156]]\n",
      "[[-0.1156356 ]\n",
      " [-0.1156356 ]\n",
      " [-0.15183362]]\n"
     ]
    }
   ],
   "source": [
    "# ----------------------------------- 1(a) ----------------------------------- #\n",
    "\n",
    "nnLayer = NNLayer(dims=[2, 2, 1], theta=theta, theta_prime=theta_prime)\n",
    "grads = nnLayer.calc_grad(x, y)\n",
    "print(grads[0])\n",
    "print(grads[1])\n",
    "\n",
    "grads_numerical = nnLayer.calc_grad_numerical(x, y)\n",
    "print(grads_numerical[0])\n",
    "print(grads_numerical[1])\n",
    "\n",
    "assert np.allclose(grads[0], grads_numerical[0], atol=1e-4)\n",
    "assert np.allclose(grads[1], grads_numerical[1], atol=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.01938197 -0.01938197]\n",
      " [-0.03876394 -0.03876394]\n",
      " [-0.01938197 -0.01938197]]\n",
      "[[-0.14059139]\n",
      " [-0.14059139]\n",
      " [-0.18460146]]\n",
      "[[-0.01938197 -0.01938197]\n",
      " [-0.03876394 -0.03876394]\n",
      " [-0.01938197 -0.01938197]]\n",
      "[[-0.14059139]\n",
      " [-0.14059139]\n",
      " [-0.18460146]]\n"
     ]
    }
   ],
   "source": [
    "# ----------------------------------- 1(b) ----------------------------------- #\n",
    "nnLayer = NNLayer(dims=[2, 2, 1], out_theta=lambda x: x, out_theta_prime=lambda x: np.ones_like(x))\n",
    "grads = nnLayer.calc_grad(x, y)\n",
    "print(grads[0])\n",
    "print(grads[1])\n",
    "\n",
    "grads_numerical = nnLayer.calc_grad_numerical(x, y)\n",
    "print(grads_numerical[0])\n",
    "print(grads_numerical[1])\n",
    "\n",
    "assert np.allclose(grads[0], grads_numerical[0], atol=1e-4)\n",
    "assert np.allclose(grads[1], grads_numerical[1], atol=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "d62b51597be18f93944234ad292c57717caf7c6510d2cf4ef79736ceb572ac63"
  },
  "kernelspec": {
   "display_name": "Python 3.7.4 64-bit ('base': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

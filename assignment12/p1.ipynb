{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from copy import deepcopy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dim_0 = 2\n",
    "# dim_1 = 2\n",
    "# dim_2 = 1\n",
    "\n",
    "# W1 = np.ones((dim_0+1, dim_1))\n",
    "# W2 = np.ones((dim_1+1, dim_2))\n",
    "# W = [W1, W2]\n",
    "\n",
    "theta = np.tanh  # activation function\n",
    "theta_prime = lambda x: 1 - np.tanh(x)**2  # derivative of the activation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# forward pass\n",
    "x = np.array([1,2])\n",
    "y = 1\n",
    "\n",
    "class NNLayer:\n",
    "    def __init__(self,\n",
    "                 dims=[2, 2, 1],\n",
    "                 theta=np.tanh,\n",
    "                 theta_prime=lambda x: 1 - np.tanh(x)**2) -> None:\n",
    "        self.W = []\n",
    "        for i in range(len(dims)-1):\n",
    "            self.W.append(np.ones((dims[i]+1, dims[i+1])))\n",
    "            # self.W.append(np.random.random((dims[i]+1, dims[i+1])))\n",
    "\n",
    "        self.theta = theta\n",
    "        self.theta_prime = theta_prime\n",
    "\n",
    "    def forward(self, x, W=None):\n",
    "        # first forward pass\n",
    "        if W is None:\n",
    "            W = self.W\n",
    "        num_layer = len(W)\n",
    "        a = x\n",
    "        self.input = x\n",
    "        s_list = []\n",
    "        a_list = []\n",
    "        for i in range(num_layer):\n",
    "            s = np.dot(np.append(a, 1), W[i])\n",
    "            a = self.theta(s)\n",
    "            s_list.append(s)\n",
    "            a_list.append(a)\n",
    "\n",
    "        self.a_list = a_list\n",
    "        self.s_list = s_list\n",
    "        return a\n",
    "\n",
    "    def calc_loss(self, pred, y):\n",
    "        # calculate loss\n",
    "        e_in = np.mean(1/4. * (pred - y)**2)\n",
    "        return e_in\n",
    "\n",
    "    def calc_grad(self, x, y):\n",
    "        pred = self.forward(x)\n",
    "\n",
    "        self.d_list = []\n",
    "\n",
    "        # compute sensitivity\n",
    "        d = 1. / 4 * 2 * (pred - y) * self.theta_prime(self.s_list[-1])\n",
    "        self.d_list.append(d)\n",
    "        for l in range(len(self.W) - 2, -1, -1):\n",
    "            # print(l)\n",
    "            d = self.W[l+1][:-1].dot(d) * self.theta_prime(self.s_list[l])\n",
    "            self.d_list.append(d)\n",
    "        self.d_list = list(reversed(self.d_list))\n",
    "\n",
    "        # gradient descent: calculate gradient G1, G2\n",
    "        self.G_list = []\n",
    "        for i in range(len(self.W)):\n",
    "            if i == 0:\n",
    "                a = self.input\n",
    "            else:\n",
    "                a = self.a_list[i-1]\n",
    "            a = np.append(a, 1)\n",
    "            G = a[:, None] @ self.d_list[i][:, None].T\n",
    "            self.G_list.append(G)\n",
    "            # G2 = a1[:, None] @ d2[:, None].T\n",
    "        return self.G_list\n",
    "\n",
    "    def calc_grad_numerical(self, x, y, eps=1e-4):\n",
    "        G_list = []\n",
    "\n",
    "        for l in range(len(self.W)):\n",
    "            r, c = self.W[l].shape\n",
    "            G = np.zeros((r, c))\n",
    "            for i in range(r):\n",
    "                for j in range(c):\n",
    "                    W_copy = deepcopy(self.W)\n",
    "                    W_copy[l][i, j] += eps\n",
    "                    pred_plus = self.forward(x, W_copy)\n",
    "                    loss_plus = self.calc_loss(pred_plus, y)\n",
    "                    W_copy[l][i, j] -= 2 * eps\n",
    "                    pred_minus = self.forward(x, W_copy)\n",
    "                    loss_minus = self.calc_loss(pred_minus, y)\n",
    "                    G[i, j] = (loss_plus - loss_minus) / (2 * eps)\n",
    "            G_list.append(G)\n",
    "        return G_list\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------------- 1(a) ----------------------------------- #\n",
    "\n",
    "nnLayer = NNLayer(dims=[2, 2, 1], theta=theta, theta_prime=theta_prime)\n",
    "grads = nnLayer.calc_grad(x, y)\n",
    "# print(grads[0])\n",
    "# print(grads[1])\n",
    "\n",
    "grads_numerical = nnLayer.calc_grad_numerical(x, y)\n",
    "# print(grads_numerical[0])\n",
    "# print(grads_numerical[1])\n",
    "\n",
    "assert np.allclose(grads[0], grads_numerical[0], atol=1e-4)\n",
    "assert np.allclose(grads[1], grads_numerical[1], atol=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------------------- 1(b) ----------------------------------- #\n",
    "nnLayer = NNLayer(dims=[2, 2, 1], theta=lambda x: x, theta_prime=lambda x: np.ones_like(x))\n",
    "grads = nnLayer.calc_grad(x, y)\n",
    "# print(grads[0])\n",
    "# print(grads[1])\n",
    "\n",
    "grads_numerical = nnLayer.calc_grad_numerical(x, y)\n",
    "# print(grads_numerical[0])\n",
    "# print(grads_numerical[1])\n",
    "\n",
    "assert np.allclose(grads[0], grads_numerical[0], atol=1e-4)\n",
    "assert np.allclose(grads[1], grads_numerical[1], atol=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.19999997 0.19999997]\n",
      " [0.19999993 0.19999993]\n",
      " [0.19999997 0.19999997]]\n",
      "[[-3.28881367e-08 -3.28881367e-08]\n",
      " [-6.57757182e-08 -6.57757182e-08]\n",
      " [-3.28881367e-08 -3.28881367e-08]]\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_22926/3815011976.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[0;31m# print(grads_numerical[1])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 103\u001b[0;31m \u001b[0;32massert\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mallclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrads\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrads_numerical\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0matol\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1e-4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    104\u001b[0m \u001b[0;32massert\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mallclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrads\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrads_numerical\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0matol\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1e-4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# forward pass\n",
    "x = np.array([1,2])\n",
    "y = 1\n",
    "\n",
    "class NNLayer:\n",
    "    def __init__(self,\n",
    "                 dims=[2, 2, 1],\n",
    "                 weight_decay=0,\n",
    "                 theta=np.tanh,\n",
    "                 theta_prime=lambda x: 1 - np.tanh(x)**2) -> None:\n",
    "        self.W = []\n",
    "        for i in range(len(dims)-1):\n",
    "            self.W.append(np.ones((dims[i]+1, dims[i+1])))\n",
    "\n",
    "        self.theta = theta\n",
    "        self.theta_prime = theta_prime\n",
    "\n",
    "        self.weight_decay = weight_decay\n",
    "\n",
    "    def forward(self, x, W=None):\n",
    "        # first forward pass\n",
    "        if W is None:\n",
    "            W = self.W\n",
    "        num_layer = len(W)\n",
    "        a = x\n",
    "        self.input = x\n",
    "        s_list = []\n",
    "        a_list = []\n",
    "        for i in range(num_layer):\n",
    "            s = np.dot(np.append(a, 1), W[i])\n",
    "            a = self.theta(s)\n",
    "            s_list.append(s)\n",
    "            a_list.append(a)\n",
    "\n",
    "        self.a_list = a_list\n",
    "        self.s_list = s_list\n",
    "        return a\n",
    "\n",
    "    def calc_loss(self, pred, y):\n",
    "        # calculate loss\n",
    "        e_in = np.mean(1/4. * (pred - y)**2)\n",
    "\n",
    "        _sum = sum(np.sum(x**2) for x in self.W)\n",
    "        e_in += self.weight_decay * _sum\n",
    "        return e_in\n",
    "\n",
    "    def calc_grad(self, x, y):\n",
    "        pred = self.forward(x)\n",
    "\n",
    "        self.d_list = []\n",
    "\n",
    "        # compute sensitivity\n",
    "        d = 1. / 4 * 2 * (pred - y) * self.theta_prime(self.s_list[-1])\n",
    "        self.d_list.append(d)\n",
    "        for l in range(len(self.W) - 2, -1, -1):\n",
    "            # print(l)\n",
    "            d = self.W[l+1][:-1].dot(d) * self.theta_prime(self.s_list[l])\n",
    "            self.d_list.append(d)\n",
    "        self.d_list = list(reversed(self.d_list))\n",
    "\n",
    "        # gradient descent: calculate gradient G1, G2\n",
    "        G_list = []\n",
    "        for i in range(len(self.W)):\n",
    "            if i == 0:\n",
    "                a = self.input\n",
    "            else:\n",
    "                a = self.a_list[i-1]\n",
    "            a = np.append(a, 1)\n",
    "            G = a[:, None] @ self.d_list[i][:, None].T\n",
    "            G_list.append(G + 2 * self.weight_decay * self.W[i])\n",
    "            # G2 = a1[:, None] @ d2[:, None].T\n",
    "        return G_list\n",
    "\n",
    "    def calc_grad_numerical(self, x, y, eps=1e-4):\n",
    "        G_list = []\n",
    "\n",
    "        for l in range(len(self.W)):\n",
    "            r, c = self.W[l].shape\n",
    "            G = np.zeros((r, c))\n",
    "            for i in range(r):\n",
    "                for j in range(c):\n",
    "                    W_copy = deepcopy(self.W)\n",
    "                    W_copy[l][i, j] += eps\n",
    "                    pred_plus = self.forward(x, W_copy)\n",
    "                    loss_plus = self.calc_loss(pred_plus, y)\n",
    "                    W_copy[l][i, j] -= 2 * eps\n",
    "                    pred_minus = self.forward(x, W_copy)\n",
    "                    loss_minus = self.calc_loss(pred_minus, y)\n",
    "                    G[i, j] = (loss_plus - loss_minus) / (2 * eps)\n",
    "            G_list.append(G)\n",
    "        return G_list\n",
    "\n",
    "\n",
    "nnLayer = NNLayer(dims=[2, 2, 1], weight_decay=0.1)\n",
    "grads = nnLayer.calc_grad(x, y)\n",
    "print(grads[0])\n",
    "# print(grads[1])\n",
    "\n",
    "grads_numerical = nnLayer.calc_grad_numerical(x, y)\n",
    "print(grads_numerical[0])\n",
    "# print(grads_numerical[1])\n",
    "\n",
    "assert np.allclose(grads[0], grads_numerical[0], atol=1e-4)\n",
    "assert np.allclose(grads[1], grads_numerical[1], atol=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[1., 1.],\n",
       "        [1., 1.],\n",
       "        [1., 1.]]),\n",
       " array([[1.],\n",
       "        [1.],\n",
       "        [1.]])]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nnLayer.W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "d62b51597be18f93944234ad292c57717caf7c6510d2cf4ef79736ceb572ac63"
  },
  "kernelspec": {
   "display_name": "Python 3.7.4 64-bit ('base': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
